{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTcqIKTnPD+0szCfSA3VQX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EderLara/CuadernosPythonParaML/blob/main/Tutorial_Polars_Elemental.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **POLARS - Introducción a Polars**\n",
        "\n",
        "## **¿Qué es Polars?**\n",
        "\n",
        "Polars es una biblioteca de manipulación de DataFrames escrita en Rust (un lenguaje de programación conocido por su seguridad y velocidad) y que utiliza Apache Arrow como su modelo de memoria columnar en segundo plano.\n",
        "\n",
        "Está diseñada desde cero para el procesamiento en paralelo y la optimización de consultas, lo que la hace significativamente más rápida que Pandas en muchas operaciones, especialmente con datasets grandes.\n",
        "\n",
        "## **¿Por qué Polars? (Ventajas y Cuándo Considerarlo)**\n",
        "\n",
        "Si bien Pandas es fantástico, Polars brilla en ciertos escenarios:\n",
        "\n",
        "1. **Rendimiento Superior:**\n",
        "    * **Velocidad:** Al estar construido en Rust y diseñado para el paralelismo (aprovecha múltiples núcleos de tu CPU automáticamente), Polars puede ser órdenes de magnitud más rápido que Pandas para muchas operaciones (filtrado, agrupaciones, joins).\n",
        "    * **Manejo Eficiente de Memoria:** Utiliza Apache Arrow, que es un formato columnar eficiente, y tiene estrategias para minimizar el uso de memoria y las copias de datos.\n",
        "2. **API Expresiva y Moderna:**\n",
        "    * **Encadenamiento de Métodos (Method Chaining):** La sintaxis de Polars se presta muy bien a encadenar operaciones de forma clara y legible.\n",
        "    * **\"Expression API\" (API de Expresiones): **Este es uno de los conceptos más poderosos de Polars. Te permite definir operaciones complejas sobre columnas de una manera muy flexible y optimizable. Veremos esto en detalle.\n",
        "    * **Lazy Evaluation (Evaluación Perezosa):** Por defecto, muchas operaciones en Polars se construyen como un \"plan de consulta\" (modo lazy) y solo se ejecutan cuando es estrictamente necesario (por ejemplo, al llamar a .collect()). Esto permite a Polars optimizar toda la cadena de operaciones antes de la ejecución.\n",
        "\n",
        "3. **Soporte para Out-of-Core (Streaming):** Polars puede procesar datasets que son más grandes que la memoria RAM disponible utilizando su API de scan_* (ej. scan_csv, scan_parquet). En este modo, Polars procesa los datos en trozos (chunks) sin necesidad de cargarlos todos en memoria.\n",
        "4. **Tipado Estricto y Apache Arrow:** El uso de Arrow asegura una gestión de tipos de datos más robusta y eficiente, además de facilitar la interoperabilidad con otros sistemas que también usan Arrow.\n",
        "\n",
        "---\n",
        "\n",
        "## **Similitudes Conceptuales con Pandas:**\n",
        "Si vienes de Pandas, muchos conceptos te resultarán familiares:\n",
        "\n",
        "* **DataFrame:** La estructura tabular principal.\n",
        "* **Series:** Representa una columna individual.\n",
        "* Operaciones como **selección**, **filtrado**, **agrupación**, **joins**, etc., existen en Polars, aunque la sintaxis para lograrlas puede diferir.\n",
        "\n",
        "## **Diferencias Clave con Pandas (a Alto Nivel):**\n",
        "\n",
        "1. **Índices (Indexes):** Polars no tiene un concepto de índice de fila mutable como el de Pandas (ej. `df.loc[]` basado en etiquetas de índice). Las filas se identifican principalmente por su posición entera. Esto simplifica la API en algunos aspectos y la hace más performante, ya que no hay que mantener la sobrecarga de un índice. Las operaciones se centran más en las columnas y sus valores.\n",
        "2. **Mutabilidad:** Polars favorece fuertemente la inmutabilidad. La mayoría de las operaciones devuelven un nuevo DataFrame o Serie, en lugar de modificar el original \"in-place\". Esto ayuda a prevenir efectos secundarios inesperados.\n",
        "3. **Lazy vs. Eager Execution:**\n",
        "    * **Eager (Ansioso):** Las operaciones se ejecutan inmediatamente (como en Pandas por defecto).\n",
        "    * **Lazy (Perezoso):** Las operaciones se registran en un plan y solo se ejecutan cuando se llama a `.collect()`. Esto permite a Polars aplicar optimizaciones al plan completo. El modo lazy es el más potente y recomendado para rendimiento.\n",
        "4. **Sintaxis de Selección y Manipulación:** Aunque los objetivos son los mismos, la forma de seleccionar columnas, filtrar filas y aplicar transformaciones (especialmente con la API de expresiones) es diferente y muy característica de Polars.\n",
        "\n",
        "---\n",
        "## **Instalación e Importación**\n",
        "1. Instalación Básica (instala los componentes comunes):\n",
        "```\n",
        "pip install polars\n",
        "```\n",
        "\n",
        "2. O, si quieres incluir todas las funcionalidades extra (como conectores a diferentes tipos de archivos, etc.):\n",
        "```\n",
        "pip install polars[all]\n",
        "```\n",
        "\n",
        "3. Importación (La convención es importar Polars con el alias pl):\n",
        "```\n",
        "import polars as pl\n",
        "```"
      ],
      "metadata": {
        "id": "Zmp2WbPCGBaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **Tabla de Contenido**\n",
        "\n",
        "1. **Introducción:** Qué es, por qué usarlo, diferencias con Pandas.\n",
        "2. **Creación de Series y DataFrames:** `pl.Series`, `pl.DataFrame`, `pl.from_dicts`.\n",
        "3. **Inspección Básica:** `head`, `tail`, `shape`, `describe`, `dtypes`, `schema`, `glimpse`.\n",
        "4. **Selección y Filtrado:** `select`, `filter`, Expresiones `pl.col()`, `pl.selectors`.\n",
        "5. **Manipulación de Datos:** `with_columns`, `drop`, `fill_null`, `rename`, `cast`, `sort`, `map_elements`.\n",
        "6. **Agrupación y Agregación:** `group_by`, `agg` con expresiones, `pivot`.\n",
        "7. **Entrada/Salida (I/O):** `read_csv`/`scan_csv`, `read_excel`, `read_parquet`/`scan_parquet`, y sus contrapartes `write_`*. Interoperabilidad."
      ],
      "metadata": {
        "id": "E9nQTAqZJukg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creación de Series y DataFrames**\n",
        "\n",
        "## **Creando Series en Polars (`pl.Series`)**\n",
        "\n",
        "Una Serie en Polars es similar a la de Pandas: un array unidimensional de datos.\n",
        "\n",
        "1. **Desde una lista de Python:**"
      ],
      "metadata": {
        "id": "F94jaWVeJvjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "\n",
        "# Creando una Serie desde una lista, especificando el nombre\n",
        "datos_lista_pl = [10, 20, 30, 40, 50]\n",
        "serie_pl_lista = pl.Series(name=\"mis_numeros\", values=datos_lista_pl)\n",
        "print(\"Serie de Polars desde una lista:\")\n",
        "print(serie_pl_lista)\n",
        "print(\"\\nTipo de dato de la Serie:\", serie_pl_lista.dtype)\n",
        "print(\"Nombre de la Serie:\", serie_pl_lista.name)\n",
        "print(\"Longitud de la Serie:\", len(serie_pl_lista)) # o serie_pl_lista.len()\n",
        "\n",
        "\"\"\"\n",
        "# Salida Esperada:\n",
        "\n",
        "Serie de Polars desde una lista:\n",
        "shape: (5,)\n",
        "Series: 'mis_numeros' [i64]\n",
        "[\n",
        "    10\n",
        "    20\n",
        "    30\n",
        "    40\n",
        "    50\n",
        "]\n",
        "\n",
        "Tipo de dato de la Serie: Int64\n",
        "Nombre de la Serie: mis_numeros\n",
        "Longitud de la Serie: 5\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "jWqd3dIMKK4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Especificando el tipo de dato (`dtype`):**\n",
        "\n",
        "Polars tiene su propio sistema de tipos de datos (ej. pl.Int32, pl.Float64, pl.Utf8 para strings, pl.Boolean, pl.Date, pl.Datetime)."
      ],
      "metadata": {
        "id": "C3TuODsTKeGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "\n",
        "datos_strings = [\"manzana\", \"banana\", \"pera\"]\n",
        "# Creando una serie de strings (Utf8 es el tipo para texto en Polars)\n",
        "serie_strings = pl.Series(name=\"frutas\", values=datos_strings, dtype=pl.Utf8)\n",
        "print(\"\\nSerie de Polars de strings:\")\n",
        "print(serie_strings)\n",
        "\n",
        "datos_floats = [1.0, 2.5, 3.0]\n",
        "serie_floats = pl.Series(name=\"decimales\", values=datos_floats, dtype=pl.Float32) # especificamos Float32\n",
        "print(\"\\nSerie de Polars de floats (Float32):\")\n",
        "print(serie_floats)"
      ],
      "metadata": {
        "id": "2fxjLGtLNkcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creando DataFrames en Polars (`pl.DataFrame`)**\n",
        "\n",
        "1. **Desde un diccionario de Python (listas, pl.Series, arrays de NumPy):**\n",
        "\n",
        "Esta es la forma más común. Las claves del diccionario son los nombres de las columnas y los valores son los datos de esas columnas."
      ],
      "metadata": {
        "id": "BaoYJDr0Nnr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import numpy as np\n",
        "\n",
        "datos_df_pl = {\n",
        "    'ID_Producto': [101, 102, 103, 104],\n",
        "    'Nombre_Producto': ['Teclado', 'Mouse', 'Monitor', 'Webcam'],\n",
        "    'Precio': [75.0, 25.5, 300.0, 45.99],\n",
        "    'Stock': np.array([50, 120, 30, 75]) # Podemos usar arrays de NumPy\n",
        "}\n",
        "df_pl = pl.DataFrame(datos_df_pl)\n",
        "print(\"\\nDataFrame de Polars desde un diccionario:\")\n",
        "print(df_pl)\n",
        "print(\"\\nSchema del DataFrame:\") # El schema es como el .info() pero más enfocado en tipos\n",
        "print(df_pl.schema)\n",
        "\n",
        "\"\"\"\n",
        "DataFrame de Polars desde un diccionario:\n",
        "shape: (4, 4)\n",
        "┌─────────────┬─────────────────┬────────┬───────┐\n",
        "│ ID_Producto ┆ Nombre_Producto ┆ Precio ┆ Stock │\n",
        "│ ---         ┆ ---             ┆ ---    ┆ ---   │\n",
        "│ i64         ┆ str             ┆ f64    ┆ i64   │\n",
        "╞═════════════╪═════════════════╪════════╪═══════╡\n",
        "│ 101         ┆ Teclado         ┆ 75.0   ┆ 50    │\n",
        "│ 102         ┆ Mouse           ┆ 25.5   ┆ 120   │\n",
        "│ 103         ┆ Monitor         ┆ 300.0  ┆ 30    │\n",
        "│ 104         ┆ Webcam          ┆ 45.99  ┆ 75    │\n",
        "└─────────────┴─────────────────┴────────┴───────┘\n",
        "\n",
        "Schema del DataFrame:\n",
        "OrderedDict([('ID_Producto', Int64), ('Nombre_Producto', Utf8), ('Precio', Float64), ('Stock', Int64)])\n",
        "\n",
        "Polars también infiere los tipos aquí.\n",
        "La representación visual del DataFrame en la consola es muy útil.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "h5tMNkZZN5RF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Definiendo el Schema explícitamente:**\n",
        "\n",
        "Se pueden definir los tipos de datos de cada columna al crear el DataFrame para mayor control o para optimizar."
      ],
      "metadata": {
        "id": "KNo_W-kiOE7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "\n",
        "datos_para_schema = {\n",
        "    'col_a': [1, 2, 3],\n",
        "    'col_b': [True, False, True],\n",
        "    'col_c': ['x', 'y', 'z']\n",
        "}\n",
        "mi_schema = {\n",
        "    'col_a': pl.Int16, # Usamos un entero más pequeño\n",
        "    'col_b': pl.Boolean,\n",
        "    'col_c': pl.Utf8\n",
        "}\n",
        "df_con_schema = pl.DataFrame(data=datos_para_schema, schema=mi_schema)\n",
        "print(\"\\nDataFrame de Polars con schema explícito:\")\n",
        "print(df_con_schema)\n",
        "print(\"\\nSchema resultante:\")\n",
        "print(df_con_schema.schema)"
      ],
      "metadata": {
        "id": "QfALIVWMOwlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Desde una lista de diccionarios (usando pl.from_dicts()):**\n",
        "\n",
        "* Cada diccionario representa una fila.\n",
        "* Polars manejará las claves faltantes introduciendo valores nulos (null en Polars, análogo a NaN en Pandas para números o None para objetos)."
      ],
      "metadata": {
        "id": "ALr2CQ2SO9Uc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "\n",
        "lista_de_diccionarios_pl = [\n",
        "    {'Nombre': 'Carlos', 'Edad': 25, 'Profesion': 'Ingeniero'},\n",
        "    {'Nombre': 'Laura', 'Edad': 30, 'Profesion': 'Doctora'},\n",
        "    {'Nombre': 'Pedro', 'Edad': 22, 'Ciudad': 'Bogotá'} # Clave 'Ciudad' nueva, 'Profesion' puede faltar\n",
        "]\n",
        "df_desde_lista_dic_pl = pl.from_dicts(lista_de_diccionarios_pl)\n",
        "print(\"\\nDataFrame de Polars desde una lista de diccionarios:\")\n",
        "print(df_desde_lista_dic_pl)\n",
        "print(\"\\nSchema resultante:\")\n",
        "print(df_desde_lista_dic_pl.schema)"
      ],
      "metadata": {
        "id": "rGcqem0LPD6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Leyendo desde archivos (adelanto):**\n",
        "\n",
        "Al igual que Pandas, Polars puede leer de varios formatos. Suelen ser más rápidas y eficientes, especialmente `read_parquet` y `scan_csv` (para modo lazy).\n",
        "\n",
        "```\n",
        "# Conceptualmente (no se ejecutará sin un archivo real):\n",
        "df_csv_polars = pl.read_csv(\"ruta/a/tu/archivo.csv\")\n",
        "df_parquet_polars = pl.read_parquet(\"ruta/a/tu/archivo.parquet\")\n",
        "\n",
        "# Usando scan_csv para modo lazy (se explicará más adelante):\n",
        "df_lazy_csv = pl.scan_csv(\"ruta/a/tu/archivo_grande.csv\")\n",
        "df_resultado = df_lazy_csv.filter(pl.col(\"columna\") > 10).collect()\n",
        "```\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "cP4db9CIPFfI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Inspección y Exploración Básica de Datos**\n",
        "\n",
        "Al igual que con Pandas, una vez que tienes un DataFrame en Polars (ya sea creado o cargado), querrás entender su estructura, los tipos de datos que contiene, y obtener una visión general de su contenido. Polars ofrece un conjunto de atributos y métodos muy eficientes para esto.\n",
        "\n",
        "Primero, creemos un DataFrame de Polars para nuestros ejemplos de inspección. Lo haremos similar al que usamos con Pandas para que puedas comparar, pero usando las convenciones de Polars."
      ],
      "metadata": {
        "id": "qJx89pwgPo6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "from datetime import date, datetime, timedelta # Para crear fechas y datetimes\n",
        "\n",
        "# Creando un DataFrame de Polars de ejemplo\n",
        "datos_polars_inspeccion = {\n",
        "    'ID_Sensor': [1, 2, 3, 4, 5, 6, 7, 8],\n",
        "    'Tipo_Sensor': ['Temp', 'Hum', 'Pres', 'Temp', 'Luz', 'Hum', 'Temp', None], # String con un None\n",
        "    'Lectura_Valor': [22.5, 65.2, 1012.5, 23.1, 800.0, 66.0, None, 10.5], # Float con un None\n",
        "    'Fecha_Lectura': [\n",
        "        date(2025, 5, 10), date(2025, 5, 10), date(2025, 5, 11), date(2025, 5, 11),\n",
        "        date(2025, 5, 12), date(2025, 5, 12), date(2025, 5, 13), date(2025, 5, 13)\n",
        "    ],\n",
        "    'Hora_Lectura': [\n",
        "        datetime(2025, 5, 10, 10, 0, 0), datetime(2025, 5, 10, 10, 5, 0),\n",
        "        datetime(2025, 5, 11, 14, 30, 0), datetime(2025, 5, 11, 14, 32, 0),\n",
        "        datetime(2025, 5, 12, 8, 0, 0), None, # Datetime con un None\n",
        "        datetime(2025, 5, 13, 18, 0, 0), datetime(2025, 5, 13, 18, 3, 0)\n",
        "    ],\n",
        "    'Activo': [True, True, False, True, True, False, True, False]\n",
        "}\n",
        "\n",
        "# Definimos el schema para mejor control de tipos, especialmente para Date y Datetime\n",
        "schema_definido = {\n",
        "    'ID_Sensor': pl.Int32,\n",
        "    'Tipo_Sensor': pl.Utf8,      # Tipo para strings\n",
        "    'Lectura_Valor': pl.Float32,\n",
        "    'Fecha_Lectura': pl.Date,\n",
        "    'Hora_Lectura': pl.Datetime, # Acepta microsegundos por defecto, podemos especificar 'us' o 'ms'\n",
        "    'Activo': pl.Boolean\n",
        "}\n",
        "\n",
        "df_pl_ins = pl.DataFrame(datos_polars_inspeccion, schema=schema_definido)\n",
        "\n",
        "print(\"DataFrame de Polars para inspección:\")\n",
        "print(df_pl_ins)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "t5LiqporQB6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. **`head(n)` y `tail(n)`: Ver las primeras/últimas filas:**\n",
        "Funcionan de manera muy similar a Pandas. Por defecto muestran 5 filas."
      ],
      "metadata": {
        "id": "aMSRTftdQF0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Primeras 3 filas del DataFrame (df_pl_ins.head(3)):\")\n",
        "print(df_pl_ins.head(3))\n",
        "\n",
        "print(\"\\nÚltimas 2 filas del DataFrame (df_pl_ins.tail(2)):\")\n",
        "print(df_pl_ins.tail(2))\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "QHVHyeXwQYpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. **`shape`: Dimensiones del DataFrame**\n",
        "Un atributo que devuelve una tupla (número_de_filas, número_de_columnas)."
      ],
      "metadata": {
        "id": "qWSIIktxQeH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dimensiones del DataFrame (filas, columnas) (df_pl_ins.shape):\")\n",
        "print(df_pl_ins.shape)\n",
        "print(f\"El DataFrame tiene {df_pl_ins.shape[0]} filas y {df_pl_ins.shape[1]} columnas.\")\n",
        "\n",
        "# También puedes obtener alto y ancho por separado:\n",
        "print(f\"Altura (número de filas): {df_pl_ins.height}\")\n",
        "print(f\"Anchura (número de columnas): {df_pl_ins.width}\")\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "usfnnxeyQqdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. **`describe()`: Estadísticas descriptivas**\n",
        "\n",
        "Proporciona un resumen estadístico de las columnas. Para columnas numéricas, incluye count (no nulos), null_count, mean, std, min, percentiles (25%, 50%, 75%) y max. Para columnas no numéricas (como strings), muestra count, null_count, unique, y mode (el valor más frecuente).\n",
        "\n"
      ],
      "metadata": {
        "id": "O9p1Zm12QtSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Estadísticas descriptivas del DataFrame (df_pl_ins.describe()):\")\n",
        "print(df_pl_ins.describe())\n",
        "# Nota: Polars describe() intenta ser inteligente y puede mostrar diferentes cosas\n",
        "# según el tipo de dato de la columna, incluyendo strings.\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "La salida de describe() en Polars es bastante completa y se adapta bien a diferentes tipos de datos.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "lTemzkHVSsG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. **`dtypes`: Tipos de datos de cada columna**\n",
        "Un atributo que devuelve una lista de los tipos de datos de Polars para cada columna."
      ],
      "metadata": {
        "id": "qW0Y4COjSvEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tipos de datos de cada columna (df_pl_ins.dtypes):\")\n",
        "print(df_pl_ins.dtypes)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "\"\"\"\n",
        "Salida Esperada:\n",
        "\n",
        "[Int32, Utf8, Float32, Date, Datetime(time_unit='us', time_zone=None), Boolean]\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "njkzTc4lUMGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. **`schema`: Schema del DataFrame**\n",
        "Un atributo que devuelve un diccionario ordenado (o similar) mapeando los nombres de las columnas a sus tipos de datos de Polars. Es más detallado que dtypes porque muestra los nombres de las columnas junto con los tipos."
      ],
      "metadata": {
        "id": "SoSwh2IDUZIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Schema del DataFrame (df_pl_ins.schema):\")\n",
        "print(df_pl_ins.schema)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "\"\"\"\n",
        "Salida Esperada:\n",
        "\n",
        "OrderedDict({'ID_Sensor': Int32, 'Tipo_Sensor': Utf8, 'Lectura_Valor': Float32, 'Fecha_Lectura': Date, 'Hora_Lectura': Datetime(time_unit='us', time_zone=None), 'Activo': Boolean})\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "p3FAksBicmIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. **`columns`: Nombres de las columnas:**\n",
        "Un atributo que devuelve una lista con los nombres de todas las columnas."
      ],
      "metadata": {
        "id": "SpMGFmA3NybT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Nombres de las columnas (df_pl_ins.columns):\")\n",
        "print(df_pl_ins.columns)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "MigOI5MSN7n8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. **`glimpse()`: Un vistazo rápido al DataFrame:**\n",
        "Este método es específico de Polars y es muy útil. Muestra una vista transpuesta del DataFrame, listando cada columna, su tipo de dato, y los primeros valores. Es como una combinación de info() y head() pero más compacto y orientado a la estructura."
      ],
      "metadata": {
        "id": "KYfAv5kkN8cZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vistazo al DataFrame (df_pl_ins.glimpse()):\")\n",
        "df_pl_ins.glimpse() # glimpse() imprime directamente, no devuelve un objeto para imprimir\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "\"\"\"\n",
        "  Salida Esperada:\n",
        "\n",
        "Vistazo al DataFrame (df_pl_ins.glimpse()):\n",
        "Rows: 8\n",
        "Columns: 6\n",
        "$ ID_Sensor     <i32> 1, 2, 3, 4, 5, 6, 7, 8\n",
        "$ Tipo_Sensor   <str> 'Temp', 'Hum', 'Pres', 'Temp', 'Luz', 'Hum', 'Temp', None\n",
        "$ Lectura_Valor <f32> 22.5, 65.2, 1012.5, 23.1, 800.0, 66.0, None, 10.5\n",
        "$ Fecha_Lectura <date> 2025-05-10, 2025-05-10, 2025-05-11, 2025-05-11, 2025-05-12, 2025-05-12, 2025-05-13, 2025-05-13\n",
        "$ Hora_Lectura  <datetime[μs]> 2025-05-10 10:00:00, 2025-05-10 10:05:00, 2025-05-11 14:30:00, 2025-05-11 14:32:00, 2025-05-12 08:00:00, None, 2025-05-13 18:00:00, 2025-05-13 18:03:00\n",
        "$ Activo        <bool> true, true, false, true, true, false, true, false\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "31ErNo8rOiKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. **`is_empty()`: Verificar si el DataFrame está vacío**"
      ],
      "metadata": {
        "id": "mLzp8Dj-OuDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"¿El DataFrame está vacío? (df_pl_ins.is_empty()): {df_pl_ins.is_empty()}\")\n",
        "df_vacio = pl.DataFrame()\n",
        "print(f\"¿Un DataFrame vacío lo está? (df_vacio.is_empty()): {df_vacio.is_empty()}\")\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "TfGFhmgqOyRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. **Operaciones en Series: `is_unique()`, `n_unique()`, `value_counts()`**\n",
        "Estas operaciones se aplican a una columna (Serie) del DataFrame."
      ],
      "metadata": {
        "id": "sYvZIN_bO3OW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionar una columna (Serie)\n",
        "col_tipo_sensor = df_pl_ins['Tipo_Sensor']\n",
        "print(f\"Columna 'Tipo_Sensor':\\n{col_tipo_sensor}\")\n",
        "\n",
        "# ¿Son todos los valores en 'Tipo_Sensor' únicos?\n",
        "print(f\"\\n¿Son únicos los valores en 'Tipo_Sensor'? {col_tipo_sensor.is_unique().all()}\") # is_unique() devuelve una serie booleana, .all() la reduce\n",
        "\n",
        "# Número de valores únicos en 'Tipo_Sensor'\n",
        "print(f\"Número de valores únicos en 'Tipo_Sensor': {col_tipo_sensor.n_unique()}\")\n",
        "\n",
        "# Frecuencia de cada valor en 'Tipo_Sensor'\n",
        "print(\"\\nFrecuencia de valores en 'Tipo_Sensor' (value_counts()):\")\n",
        "print(col_tipo_sensor.value_counts())\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "\"\"\"\n",
        "  Salida Esperada:\n",
        "\n",
        "Frecuencia de valores en 'Tipo_Sensor' (value_counts()):\n",
        "shape: (5, 2)\n",
        "┌─────────────┬────────┐\n",
        "│ Tipo_Sensor ┆ counts │\n",
        "│ ---         ┆ ---    │\n",
        "│ str         ┆ u32    │\n",
        "╞═════════════╪════════╡\n",
        "│ Temp        ┆ 3      │\n",
        "│ Hum         ┆ 2      │\n",
        "│ Luz         ┆ 1      │\n",
        "│ null        ┆ 1      │\n",
        "│ Pres        ┆ 1      │\n",
        "└─────────────┴────────┘\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "IN2OKacgPB51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Selección y Filtrado de Datos**\n",
        "\n",
        "Esta es una de las áreas donde Polars realmente muestra su poder y su enfoque diferente, principalmente a través de su API de Expresiones. Las expresiones te permiten construir operaciones complejas de forma declarativa, que Polars luego puede optimizar, especialmente en modo lazy (aunque también funcionan en modo eager)\n",
        "\n",
        "Seguimos usando el mismo dataset:"
      ],
      "metadata": {
        "id": "CaFIZ1Q7PJyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "from datetime import date, datetime # Para nuestros datos de ejemplo\n",
        "\n",
        "# Recreamos el DataFrame anterior para consistencia\n",
        "datos_polars_inspeccion = {\n",
        "    'ID_Sensor': [1, 2, 3, 4, 5, 6, 7, 8],\n",
        "    'Tipo_Sensor': ['Temp', 'Hum', 'Pres', 'Temp', 'Luz', 'Hum', 'Temp', None],\n",
        "    'Lectura_Valor': [22.5, 65.2, 1012.5, 23.1, 800.0, 66.0, None, 10.5],\n",
        "    'Fecha_Lectura': [\n",
        "        date(2025, 5, 10), date(2025, 5, 10), date(2025, 5, 11), date(2025, 5, 11),\n",
        "        date(2025, 5, 12), date(2025, 5, 12), date(2025, 5, 13), date(2025, 5, 13)\n",
        "    ],\n",
        "    'Hora_Lectura': [\n",
        "        datetime(2025, 5, 10, 10, 0, 0), datetime(2025, 5, 10, 10, 5, 0),\n",
        "        datetime(2025, 5, 11, 14, 30, 0), datetime(2025, 5, 11, 14, 32, 0),\n",
        "        datetime(2025, 5, 12, 8, 0, 0), None,\n",
        "        datetime(2025, 5, 13, 18, 0, 0), datetime(2025, 5, 13, 18, 3, 0)\n",
        "    ],\n",
        "    'Activo': [True, True, False, True, True, False, True, False]\n",
        "}\n",
        "schema_definido = {\n",
        "    'ID_Sensor': pl.Int32, 'Tipo_Sensor': pl.Utf8, 'Lectura_Valor': pl.Float32,\n",
        "    'Fecha_Lectura': pl.Date, 'Hora_Lectura': pl.Datetime, 'Activo': pl.Boolean\n",
        "}\n",
        "df_pl_ins = pl.DataFrame(datos_polars_inspeccion, schema=schema_definido)\n",
        "\n",
        "print(\"DataFrame original Selección y Filtrado de Datos:\")\n",
        "print(df_pl_ins)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "54s0ineoPvjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. **Selección de Columnas**\n",
        "\n",
        "  ### **Usando `df.select()` con Expresiones `pl.col()` (la forma más idiomática y potente):**\n",
        "\n",
        "  El método `select()` es la principal forma de elegir, transformar o crear nuevas columnas. Se usa con expresiones de Polars, comúnmente pl.col(\"nombre_columna\") para referirse a una columna existente."
      ],
      "metadata": {
        "id": "snNANaRARCJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Selección de Columnas con df.select() ---\")\n",
        "# Seleccionar una sola columna\n",
        "df_una_columna = df_pl_ins.select(pl.col(\"Tipo_Sensor\"))\n",
        "print(\"\\nSelección de una columna ('Tipo_Sensor'):\")\n",
        "print(df_una_columna)\n",
        "\n",
        "# Seleccionar múltiples columnas\n",
        "df_multiples_columnas = df_pl_ins.select(\n",
        "    pl.col(\"ID_Sensor\"),\n",
        "    pl.col(\"Fecha_Lectura\"),\n",
        "    pl.col(\"Lectura_Valor\")\n",
        ")\n",
        "print(\"\\nSelección de múltiples columnas:\")\n",
        "print(df_multiples_columnas)\n",
        "\n",
        "# Seleccionar columnas y renombrarlas con alias\n",
        "df_con_alias = df_pl_ins.select(\n",
        "    pl.col(\"ID_Sensor\").alias(\"Identificador\"),\n",
        "    pl.col(\"Lectura_Valor\").alias(\"Valor\")\n",
        ")\n",
        "print(\"\\nSelección con alias:\")\n",
        "print(df_con_alias)\n",
        "\n",
        "# Seleccionar todas las columnas\n",
        "df_todas_columnas = df_pl_ins.select(pl.all()) # pl.all() selecciona todo\n",
        "# print(\"\\nTodas las columnas (igual que el original en este caso):\")\n",
        "# print(df_todas_columnas)\n",
        "\n",
        "# Excluir columnas\n",
        "df_sin_fechas = df_pl_ins.select(pl.all().exclude(\"Fecha_Lectura\", \"Hora_Lectura\"))\n",
        "print(\"\\nTodas las columnas EXCEPTO las de fecha:\")\n",
        "print(df_sin_fechas)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "Xb1aS0RZRj2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Usando `df.select()` con strings o listas de strings (más conciso para selección simple):**\n",
        "Polars permite pasar directamente strings o listas de strings a select para una selección simple."
      ],
      "metadata": {
        "id": "MsSeOExbSjBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionar una columna pasando su nombre como string\n",
        "df_tipo_sensor_str = df_pl_ins.select(\"Tipo_Sensor\")\n",
        "print(\"Selección de 'Tipo_Sensor' usando string:\")\n",
        "print(df_tipo_sensor_str)\n",
        "\n",
        "# Seleccionar múltiples columnas pasando una lista de strings\n",
        "df_id_valor_str = df_pl_ins.select([\"ID_Sensor\", \"Lectura_Valor\", \"Activo\"])\n",
        "print(\"\\nSelección de múltiples columnas usando lista de strings:\")\n",
        "print(df_id_valor_str)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "HSpCaHzmSiQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Usando Selectores (API `pl.selectors` o cs):**\n",
        "Para selecciones más programáticas o basadas en patrones/tipos."
      ],
      "metadata": {
        "id": "SeYn16koTEoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars.selectors as cs # Alias común para los selectores\n",
        "\n",
        "# Seleccionar todas las columnas de tipo string (Utf8)\n",
        "df_columnas_string = df_pl_ins.select(cs.string()) # cs.string() es un alias para cs.by_dtype(pl.Utf8)\n",
        "print(\"Selección de todas las columnas de tipo string (Utf8):\")\n",
        "print(df_columnas_string)\n",
        "\n",
        "# Seleccionar todas las columnas numéricas (enteros y flotantes)\n",
        "df_columnas_numericas = df_pl_ins.select(cs.numeric())\n",
        "print(\"\\nSelección de todas las columnas numéricas:\")\n",
        "print(df_columnas_numericas)\n",
        "\n",
        "# Seleccionar columnas que empiezan con \"Fecha_\"\n",
        "df_columnas_fecha = df_pl_ins.select(cs.starts_with(\"Fecha_\"))\n",
        "print(\"\\nSelección de columnas que empiezan con 'Fecha_':\")\n",
        "print(df_columnas_fecha)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "JCm6SS2tTJkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Acceso directo con corchetes [ ] (similar a Pandas):**\n",
        "\n",
        "* `df[\"nombre_columna\"]` devuelve una Serie de Polars.\n",
        "* `df[[\"col1\", \"col2\"]]` devuelve un DataFrame de Polars con esas columnas."
      ],
      "metadata": {
        "id": "7QF5_zndTX6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionar una columna como Serie\n",
        "serie_tipo_sensor = df_pl_ins[\"Tipo_Sensor\"]\n",
        "print(\"Columna 'Tipo_Sensor' como Serie de Polars:\")\n",
        "print(serie_tipo_sensor)\n",
        "print(f\"Tipo: {type(serie_tipo_sensor)}\")\n",
        "\n",
        "# Seleccionar múltiples columnas como DataFrame\n",
        "df_id_y_activo = df_pl_ins[[\"ID_Sensor\", \"Activo\"]]\n",
        "print(\"\\nColumnas 'ID_Sensor' y 'Activo' como DataFrame:\")\n",
        "print(df_id_y_activo)\n",
        "print(f\"Tipo: {type(df_id_y_activo)}\")\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "v7OjSuIaTlK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. **Selección de Filas (Slicing)**\n",
        "\n",
        "Polars no tiene un índice como el de Pandas que se use con .loc o .iloc de la misma manera para la selección de filas basada en etiquetas. La selección de filas se hace principalmente por posición o condición.\n",
        "\n",
        "### **Usando d`f.slice(offset, length)`:**\n",
        "Selecciona un trozo del DataFrame especificando un desplazamiento (desde dónde empezar) y una longitud (cuántas filas tomar).\n",
        "\n",
        "\n",
        "\n",
        "> **Nota:** Recuerda que `df.head(n)` y `df.tail(n)` también son formas de seleccionar las primeras o últimas n filas.\n",
        "\n"
      ],
      "metadata": {
        "id": "zQeIYWNLTqmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Selección de Filas (Slicing) ---\")\n",
        "# Seleccionar 3 filas empezando desde la fila en posición 2 (la tercera fila)\n",
        "df_slice = df_pl_ins.slice(offset=2, length=3)\n",
        "print(\"\\nSlice de 3 filas desde la posición 2:\")\n",
        "print(df_slice)\n",
        "\n",
        "# Seleccionar las primeras 4 filas (similar a head(4) pero con slice)\n",
        "df_slice_inicio = df_pl_ins.slice(offset=0, length=4)\n",
        "# print(\"\\nSlice de las primeras 4 filas:\")\n",
        "# print(df_slice_inicio)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "-ef89SeQT228"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Acceso directo a filas por índice entero (similar a iloc para una sola fila en Pandas):**\n",
        "\n",
        "* `df[indice_fila] `devuelve la fila como un DataFrame de una fila.\n",
        "* `df[indice_inicio:indice_fin]` realiza un slicing."
      ],
      "metadata": {
        "id": "B4sBS7xoUJ96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionar la fila en la posición 0 (la primera fila)\n",
        "fila_0 = df_pl_ins[0]\n",
        "print(\"Fila en posición 0:\")\n",
        "print(fila_0)\n",
        "\n",
        "# Slicing de filas (similar a Python, el final es exclusivo)\n",
        "slice_filas_directo = df_pl_ins[2:5] # Filas en posición 2, 3, 4\n",
        "print(\"\\nSlice de filas [2:5]:\")\n",
        "print(slice_filas_directo)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "mjk0yNldU95d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. **Filtrado de Filas con `df.filter()` y Expresiones**\n",
        "\n",
        "Este es el método principal y más potente para seleccionar filas que cumplen ciertas condiciones. Se usa con la API de Expresiones de Polars."
      ],
      "metadata": {
        "id": "lPps68w2U-05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Filtrado de Filas con df.filter() ---\")\n",
        "# 1. Condición numérica: Sensores con Lectura_Valor > 50.0\n",
        "df_lecturas_altas = df_pl_ins.filter(pl.col(\"Lectura_Valor\") > 50.0)\n",
        "print(\"\\nSensores con Lectura_Valor > 50.0:\")\n",
        "print(df_lecturas_altas)\n",
        "\n",
        "# 2. Condición de string: Sensores de tipo 'Temp'\n",
        "df_tipo_temp = df_pl_ins.filter(pl.col(\"Tipo_Sensor\") == \"Temp\")\n",
        "print(\"\\nSensores de tipo 'Temp':\")\n",
        "print(df_tipo_temp)\n",
        "\n",
        "# También puedes usar métodos de string dentro de las expresiones\n",
        "df_tipo_contiene_m = df_pl_ins.filter(pl.col(\"Tipo_Sensor\").str.contains(\"m\")) # Contiene 'm' (Hum)\n",
        "print(\"\\nSensores cuyo tipo contiene la letra 'm':\")\n",
        "print(df_tipo_contiene_m)\n",
        "\n",
        "# 3. Combinar múltiples condiciones (usando & para AND, | para OR, ~ para NOT):\n",
        "# Sensores de tipo 'Temp' Y que estén 'Activo'\n",
        "df_temp_activos = df_pl_ins.filter(\n",
        "    (pl.col(\"Tipo_Sensor\") == \"Temp\") & (pl.col(\"Activo\") == True) # o simplemente pl.col(\"Activo\")\n",
        ")\n",
        "print(\"\\nSensores de tipo 'Temp' Y Activos:\")\n",
        "print(df_temp_activos)\n",
        "\n",
        "# Sensores con Lectura_Valor < 20 O Lectura_Valor > 500\n",
        "df_lecturas_extremos = df_pl_ins.filter(\n",
        "    (pl.col(\"Lectura_Valor\") < 20.0) | (pl.col(\"Lectura_Valor\") > 500.0)\n",
        ")\n",
        "print(\"\\nSensores con Lectura_Valor < 20.0 O > 500.0:\")\n",
        "print(df_lecturas_extremos)\n",
        "\n",
        "# 4. Usando `.is_in()`:\n",
        "# Sensores de tipo 'Hum' o 'Luz'\n",
        "tipos_buscados = [\"Hum\", \"Luz\"]\n",
        "df_hum_o_luz = df_pl_ins.filter(pl.col(\"Tipo_Sensor\").is_in(tipos_buscados))\n",
        "print(\"\\nSensores de tipo 'Hum' o 'Luz':\")\n",
        "print(df_hum_o_luz)\n",
        "\n",
        "# 5. Usando `.is_null()` o `.is_not_null()`:\n",
        "# Filas donde 'Lectura_Valor' es nulo\n",
        "df_lectura_nula = df_pl_ins.filter(pl.col(\"Lectura_Valor\").is_null())\n",
        "print(\"\\nFilas con 'Lectura_Valor' nulo:\")\n",
        "print(df_lectura_nula)\n",
        "\n",
        "# Filas donde 'Tipo_Sensor' NO es nulo\n",
        "df_tipo_no_nulo = df_pl_ins.filter(pl.col(\"Tipo_Sensor\").is_not_null())\n",
        "print(\"\\nFilas con 'Tipo_Sensor' NO nulo:\")\n",
        "print(df_tipo_no_nulo)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "CMGSxBkGVKXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. **Encadenamiento de Operaciones (Selección y Filtrado)**\n",
        "\n",
        "Una gran ventaja de Polars es cómo se pueden encadenar las operaciones de forma legible:"
      ],
      "metadata": {
        "id": "mo1aQ5j3VLnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Encadenamiento de Operaciones ---\")\n",
        "# Seleccionar ID_Sensor, Tipo_Sensor y Lectura_Valor para los sensores 'Temp' que estén activos\n",
        "# y cuya lectura sea mayor a 23.0\n",
        "resultado_encadenado = (\n",
        "    df_pl_ins\n",
        "    .filter(\n",
        "        (pl.col(\"Tipo_Sensor\") == \"Temp\") &\n",
        "        (pl.col(\"Activo\") == True) &\n",
        "        (pl.col(\"Lectura_Valor\") > 23.0)\n",
        "    )\n",
        "    .select([\"ID_Sensor\", \"Tipo_Sensor\", \"Lectura_Valor\"])\n",
        "    .sort(\"Lectura_Valor\", descending=True) # Añadimos un ordenamiento también\n",
        ")\n",
        "print(\"\\nResultado de filtrar y seleccionar encadenadamente:\")\n",
        "print(resultado_encadenado)"
      ],
      "metadata": {
        "id": "riBITwCeVXMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Manipulación de Datos.**\n",
        "\n",
        "Aquí es donde la \"Expression API\" de Polars realmente se luce. La mayoría de las transformaciones, creación de nuevas columnas y modificaciones se realizan de manera muy eficiente y expresiva usando el método `with_columns()` (o a` veces select()` si solo quieres un subconjunto transformado).\n",
        "\n",
        "* DataFrame para ejemplos de manipulación:"
      ],
      "metadata": {
        "id": "L9npR_SuVYA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "from datetime import date, datetime, timedelta\n",
        "import numpy as np # Para algún NaN si es necesario\n",
        "\n",
        "# DataFrame de Polars para manipulación\n",
        "datos_manipulacion = {\n",
        "    'ID_Transaccion': range(1, 9),\n",
        "    'Producto': ['A', 'B', 'A', 'C', 'B', 'A', 'C', 'B'],\n",
        "    'Fecha': [\n",
        "        date(2025, 1, 5), date(2025, 1, 5), date(2025, 1, 6), date(2025, 1, 6),\n",
        "        date(2025, 1, 7), date(2025, 1, 8), date(2025, 1, 8), date(2025, 1, 9)\n",
        "    ],\n",
        "    'Cantidad': [10, 5, 8, 12, 6, 15, 7, 9],\n",
        "    'Precio_Unitario': [100.0, 250.5, 105.0, 50.0, 260.0, None, 52.5, 240.0],   # Incluimos un None\n",
        "    'Descuento_Aplicado': [0.1, 0.0, 0.1, 0.05, 0.0, 0.15, 0.0, 0.05]\n",
        "}\n",
        "schema_manip = {\n",
        "    'ID_Transaccion': pl.Int16, 'Producto': pl.Categorical,                     # Usamos Categorical para Producto\n",
        "    'Fecha': pl.Date, 'Cantidad': pl.Int32, 'Precio_Unitario': pl.Float64,\n",
        "    'Descuento_Aplicado': pl.Float32\n",
        "}\n",
        "df_pl_manip = pl.DataFrame(datos_manipulacion, schema=schema_manip)\n",
        "\n",
        "\"\"\"\n",
        "  Usamos pl.Categorical para la columna 'Producto', que es un tipo de dato eficiente en Polars para strings con un número limitado de valores únicos.\n",
        "\"\"\"\n",
        "\n",
        "print(\"DataFrame original para Manipulación de Datos:\")\n",
        "print(df_pl_manip)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "lt3KZaxzWtU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. **Añadir y Modificar Columnas con `df.with_columns()`**\n",
        "\n",
        "`with_columns()` es el método principal para añadir nuevas columnas o transformar las existentes. Toma una o más expresiones de Polars.\n",
        "\n",
        " * Añadir una columna con un valor literal (escalar):\n",
        "  Usamos `pl.lit()` para crear una expresión a partir de un valor literal."
      ],
      "metadata": {
        "id": "hrAcEtWTZQ_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_con_literal = df_pl_manip.with_columns(\n",
        "    pl.lit(\"Tienda_Online\").alias(\"Canal_Venta\")\n",
        ")\n",
        "print(\"--- Añadir/Modificar Columnas con with_columns ---\")\n",
        "print(\"\\nDataFrame con nueva columna 'Canal_Venta' (literal):\")\n",
        "print(df_con_literal)"
      ],
      "metadata": {
        "id": "Ia3ZiZZD0fin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. **Añadir una columna derivada de otras existentes:**"
      ],
      "metadata": {
        "id": "GO4p-CgU0ddz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular Ingreso_Bruto = Cantidad * Precio_Unitario\n",
        "# Calcular Precio_Neto = Precio_Unitario * (1 - Descuento_Aplicado)\n",
        "df_con_calculos = df_pl_manip.with_columns([\n",
        "    (pl.col(\"Cantidad\") * pl.col(\"Precio_Unitario\")).alias(\"Ingreso_Bruto\"),\n",
        "    (pl.col(\"Precio_Unitario\") * (1 - pl.col(\"Descuento_Aplicado\"))).alias(\"Precio_Neto_Unitario\") # Observa que si 'Precio_Unitario' es nulo, el resultado de las operaciones también será nulo.\n",
        "])\n",
        "print(\"\\nDataFrame con 'Ingreso_Bruto' y 'Precio_Neto_Unitario':\")\n",
        "print(df_con_calculos)"
      ],
      "metadata": {
        "id": "fdmzXDQ_0ov0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. **Modificar una columna existente:**\n",
        "Si el alias (`.alias(\"nombre_columna\")`) coincide con el nombre de una columna existente, esa columna se sobrescribe."
      ],
      "metadata": {
        "id": "h_KQkQsU0ptH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir 'Producto' a minúsculas (ya es Categorical, pero .str funciona en expresiones)\n",
        "df_producto_minusculas = df_pl_manip.with_columns(\n",
        "    pl.col(\"Producto\").str.to_lowercase().alias(\"Producto\") # Sobrescribe 'Producto'\n",
        ")\n",
        "print(\"\\nDataFrame con 'Producto' en minúsculas:\")\n",
        "print(df_producto_minusculas)"
      ],
      "metadata": {
        "id": "tpdAdwZaKEPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. **Crear columnas con lógica condicional (`pl.when().then().otherwise()`):**"
      ],
      "metadata": {
        "id": "djjQQzbFKoBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorizar Descuento: 'Alto' si > 0.1, 'Medio' si > 0.05, 'Bajo' de lo contrario\n",
        "df_con_categoria_descuento = df_pl_manip.with_columns(\n",
        "    pl.when(pl.col(\"Descuento_Aplicado\") > 0.1)\n",
        "    .then(pl.lit(\"Alto\"))\n",
        "    .when(pl.col(\"Descuento_Aplicado\") > 0.05)\n",
        "    .then(pl.lit(\"Medio\"))\n",
        "    .otherwise(pl.lit(\"Bajo\"))\n",
        "    .alias(\"Nivel_Descuento\")\n",
        ")\n",
        "print(\"\\nDataFrame con 'Nivel_Descuento':\")\n",
        "print(df_con_categoria_descuento[[\"Producto\", \"Descuento_Aplicado\", \"Nivel_Descuento\"]])\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "7dMmc7eAK1kN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Eliminar Columnas con `df.drop()`**"
      ],
      "metadata": {
        "id": "xtTu29K7K2ls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Eliminar Columnas ---\")\n",
        "# Creamos una copia para no afectar el df original de esta sección\n",
        "df_para_drop = df_con_calculos.clone() # .clone() para una copia profunda\n",
        "\n",
        "# Eliminar una sola columna\n",
        "df_drop_una = df_para_drop.drop(\"Ingreso_Bruto\")\n",
        "print(\"\\nDataFrame después de eliminar 'Ingreso_Bruto':\")\n",
        "print(df_drop_una.columns)\n",
        "\n",
        "# Eliminar múltiples columnas\n",
        "df_drop_multiples = df_para_drop.drop([\"Ingreso_Bruto\", \"Precio_Neto_Unitario\"])\n",
        "print(\"\\nDataFrame después de eliminar 'Ingreso_Bruto' y 'Precio_Neto_Unitario':\")\n",
        "print(df_drop_multiples.columns)\n",
        "\n",
        "# También se puede pasar una sola columna sin lista a drop\n",
        "# df_drop_una_alt = df_para_drop.drop(\"Ingreso_Bruto\")\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "hg0xTDuyLIp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **C. Manejo de Valores Nulos (null)**\n",
        "\n",
        "## 1. **Contar nulos (repaso):**"
      ],
      "metadata": {
        "id": "k6hTIQrfLLLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Manejo de Valores Nulos ---\")\n",
        "print(\"Conteo de nulos por columna:\")\n",
        "print(df_pl_manip.null_count()) # Muestra un DataFrame con los conteos\n",
        "# O para una columna específica, dentro de una expresión:\n",
        "# print(df_pl_manip.select(pl.col(\"Precio_Unitario\").is_null().sum().alias(\"nulos_precio\")))"
      ],
      "metadata": {
        "id": "rJfnOzdHLaKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. **Eliminar filas con nulos: `df.drop_nulls()`:**"
      ],
      "metadata": {
        "id": "Jvf7qHPPMC6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminar cualquier fila que tenga al menos un valor nulo\n",
        "df_sin_nulos_filas = df_pl_manip.drop_nulls()\n",
        "print(\"\\nDataFrame después de drop_nulls() (elimina filas con cualquier nulo):\")\n",
        "print(df_sin_nulos_filas) # La fila con Precio_Unitario nulo desaparece\n",
        "\n",
        "# Eliminar filas si tienen nulos en un subconjunto de columnas\n",
        "# (En nuestro df, solo Precio_Unitario tiene nulos)\n",
        "df_sin_nulos_subset = df_pl_manip.drop_nulls(subset=[\"Precio_Unitario\"])\n",
        "print(\"\\nDataFrame después de drop_nulls(subset=['Precio_Unitario']):\")\n",
        "print(df_sin_nulos_subset)"
      ],
      "metadata": {
        "id": "NRfXCnZGMJuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. **Rellenar nulos usando expresiones con fill_null() (dentro de with_columns o select):**\n",
        "\n",
        "* Tambien existen otras estrategías para fill_null: '`backward`', '`min`', '`max`', '`one`', '`zero`'."
      ],
      "metadata": {
        "id": "ivqO15EhSBU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rellenar nulos en 'Precio_Unitario' con un valor literal (ej. 0.0)\n",
        "df_relleno_literal = df_pl_manip.with_columns(\n",
        "    pl.col(\"Precio_Unitario\").fill_null(0.0).alias(\"Precio_Rellenado_Literal\")\n",
        ")\n",
        "print(\"\\n'Precio_Unitario' con nulos rellenados con 0.0:\")\n",
        "print(df_relleno_literal[[\"Precio_Unitario\", \"Precio_Rellenado_Literal\"]])\n",
        "\n",
        "# Rellenar nulos con la media de la columna (Polars calcula la media ignorando nulos)\n",
        "df_relleno_media = df_pl_manip.with_columns(\n",
        "    pl.col(\"Precio_Unitario\").fill_null(pl.col(\"Precio_Unitario\").mean()).alias(\"Precio_Rellenado_Media\")\n",
        ")\n",
        "print(\"\\n'Precio_Unitario' con nulos rellenados con la media:\")\n",
        "print(df_relleno_media[[\"ID_Transaccion\", \"Precio_Unitario\", \"Precio_Rellenado_Media\"]])\n",
        "\n",
        "# Rellenar nulos con una estrategia (ej. 'forward' fill - ffill)\n",
        "# Ordenamos por fecha primero para que el forward fill tenga más sentido si hubiera múltiples nulos\n",
        "df_relleno_forward = df_pl_manip.sort(\"Fecha\").with_columns(\n",
        "    pl.col(\"Precio_Unitario\").fill_null(strategy=\"forward\").alias(\"Precio_Rellenado_Forward\")\n",
        ")\n",
        "print(\"\\n'Precio_Unitario' con nulos rellenados con estrategia 'forward' (después de ordenar):\")\n",
        "print(df_relleno_forward[[\"ID_Transaccion\", \"Fecha\", \"Precio_Unitario\", \"Precio_Rellenado_Forward\"]])\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "76MFmW5_SLF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Renombrar Columnas con `df.rename()`:**"
      ],
      "metadata": {
        "id": "FDHNK79ISRPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Renombrar Columnas ---\")\n",
        "df_renombrado = df_pl_manip.rename({\n",
        "    \"ID_Transaccion\": \"ID\",\n",
        "    \"Precio_Unitario\": \"Precio_Base\"\n",
        "})\n",
        "print(\"\\nDataFrame con columnas renombradas:\")\n",
        "print(df_renombrado.columns)\n",
        "print(df_renombrado.head(2))\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "uu0eVFFdSxo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Cambiar Tipos de Datos (Casting) con `cast()`**:\n",
        "Se usa como una expresión dentro de `with_columns` o `select`."
      ],
      "metadata": {
        "id": "faPHcBR4S19i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Cambiar Tipos de Datos (Casting) ---\")\n",
        "# Cambiar Cantidad a Float32 y Precio_Unitario (después de rellenar nulos) a Int32\n",
        "df_tipos_cambiados = df_pl_manip.with_columns([\n",
        "    pl.col(\"Cantidad\").cast(pl.Float32),\n",
        "    pl.col(\"Precio_Unitario\").fill_null(0).cast(pl.Int32).alias(\"Precio_Entero_NoNulo\") # Encadenamos fill_null y cast\n",
        "])\n",
        "print(\"\\nDataFrame con tipos de datos cambiados:\")\n",
        "print(df_tipos_cambiados.select([\"Producto\", \"Cantidad\", \"Precio_Entero_NoNulo\"]))\n",
        "print(df_tipos_cambiados.dtypes) # Mostrar los nuevos tipos\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "Sy9c7fHYTC5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ordenar Datos con `df.sort()`**"
      ],
      "metadata": {
        "id": "qFOUrPQjTKjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Ordenar Datos ---\")\n",
        "# Ordenar por 'Fecha' (ascendente) y luego por 'Precio_Unitario' (descendente)\n",
        "df_ordenado = df_pl_manip.sort([\"Fecha\", \"Precio_Unitario\"], descending=[False, True])\n",
        "print(\"\\nDataFrame ordenado por Fecha (asc) y Precio_Unitario (desc):\")\n",
        "print(df_ordenado)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "x8LAmpIYTQqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Aplicar Funciones Personalizadas con map_elements() (en Series/Expresiones):**\n",
        "\n",
        "Si necesitas aplicar una función Python arbitraria que no tiene un equivalente directo en las expresiones de Polars, puedes usar `map_elements()`. Ten en cuenta que esto puede ser más lento que usar expresiones nativas de Polars porque implica pasar datos entre el motor de Rust y el intérprete de Python para cada elemento.\n",
        "\n",
        "> **Nota:** `map_elements` reemplazó a `.apply()` para Series en versiones más recientes de Polars para este tipo de operación. Asegúrate de usar return_dtype."
      ],
      "metadata": {
        "id": "jXCsIt9DUGXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Aplicar Funciones Personalizadas con map_elements ---\")\n",
        "def mi_funcion_compleja_producto(nombre_producto: str) -> str:\n",
        "    if nombre_producto is None:\n",
        "        return \"PRODUCTO DESCONOCIDO\"\n",
        "    return f\"PROD-{nombre_producto.upper()}-XYZ\"\n",
        "\n",
        "# Es crucial especificar el return_dtype para map_elements\n",
        "df_con_map = df_pl_manip.with_columns(\n",
        "    pl.col(\"Producto\")\n",
        "    .map_elements(mi_funcion_compleja_producto, return_dtype=pl.Utf8)\n",
        "    .alias(\"Producto_Transformado\")\n",
        ")\n",
        "print(\"\\nDataFrame con 'Producto_Transformado' usando map_elements:\")\n",
        "print(df_con_map.select([\"Producto\", \"Producto_Transformado\"]))\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "p9zVz7cUW27p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# **5. Agrupación de Datos (`group_by`) y Agregaciones.**\n",
        "\n",
        "Al igual que en Pandas, la agrupación te permite dividir tus datos en subconjuntos basados en los valores de ciertas columnas y luego realizar cálculos o aplicar transformaciones a cada uno de estos subconjuntos. El paradigma sigue siendo Split-Apply-Combine.\n",
        "\n",
        "En Polars, esto se logra principalmente con el método group_by() seguido del **`método agg()`**, donde la API de Expresiones juega un papel central para definir las agregaciones."
      ],
      "metadata": {
        "id": "fw7LpFz-W4KP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "from datetime import date\n",
        "\n",
        "# DataFrame de Polars para agrupación y agregaciones\n",
        "datos_agrupacion = {\n",
        "    'ID_Transaccion': range(1, 11),\n",
        "    'Producto': ['A', 'B', 'A', 'C', 'B', 'A', 'C', 'B', 'A', 'B'],\n",
        "    'Region': ['Norte', 'Sur', 'Norte', 'Este', 'Sur', 'Norte', 'Este', 'Sur', 'Oeste', 'Norte'],\n",
        "    'Fecha': [\n",
        "        date(2025, 1, 5), date(2025, 1, 5), date(2025, 1, 6), date(2025, 1, 6),\n",
        "        date(2025, 1, 7), date(2025, 1, 8), date(2025, 1, 8), date(2025, 1, 9),\n",
        "        date(2025, 1, 9), date(2025, 1, 10)\n",
        "    ],\n",
        "    'Cantidad': [10, 5, 8, 12, 6, 15, 7, 9, 20, 11],\n",
        "    'Ingresos': [1000.0, 1252.5, 840.0, 600.0, 1560.0, 1575.0, 367.5, 2160.0, 2000.0, 2761.0],\n",
        "    'Valoracion': [4, 5, 3, 4, 5, 2, 3, 5, 4, 5]\n",
        "}\n",
        "schema_agrup = {\n",
        "    'ID_Transaccion': pl.Int16, 'Producto': pl.Categorical, 'Region': pl.Categorical,\n",
        "    'Fecha': pl.Date, 'Cantidad': pl.Int32, 'Ingresos': pl.Float64,\n",
        "    'Valoracion': pl.Int8\n",
        "}\n",
        "df_pl_agrup = pl.DataFrame(datos_agrupacion, schema=schema_agrup)\n",
        "\n",
        "print(\"DataFrame original para aprupación de datos:\")\n",
        "print(df_pl_agrup)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "mv021sEAX1qX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. **Usando `df.group_by()`:**\n",
        "\n",
        "El método `group_by()` por sí solo crea un objeto GroupBy. Para obtener un resultado tangible, necesitas encadenarle un método de agregación, típicamente `agg()`.\n",
        "\n",
        "Agrupar por una sola columna:"
      ],
      "metadata": {
        "id": "08R8Jg8tZuFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Agrupación con df.group_by() ---\")\n",
        "# Agrupar por 'Producto'. Esto crea un objeto GroupBy.\n",
        "gb_producto = df_pl_agrup.group_by(\"Producto\")\n",
        "print(\"Tipo de objeto después de group_by:\", type(gb_producto))\n",
        "# <class 'polars.dataframe.group_by.GroupBy'>"
      ],
      "metadata": {
        "id": "JpJZw8h3Z78-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. **Agrupar por múltiples columnas:**\n",
        "Pasando una lista de nombres de columnas."
      ],
      "metadata": {
        "id": "5f5a6ZsHZ_ST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gb_producto_region = df_pl_agrup.group_by([\"Producto\", \"Region\"])\n",
        "# print(\"\\nTipo de objeto después de group_by múltiple:\", type(gb_producto_region))"
      ],
      "metadata": {
        "id": "JMHVZiRnaEU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. **Mantener el orden original de los grupos (`maintain_order=True`):**\n",
        "Por defecto, group_by no garantiza el orden de los grupos resultantes. Si el orden es importante (por ejemplo, el orden en que aparecen los primeros grupos en tus datos originales), puedes usar `maintain_order=True.`\n",
        "\n",
        "Esto puede tener un ligero impacto en el rendimiento."
      ],
      "metadata": {
        "id": "6-fec5gYONlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gb_producto_ordenado = df_pl_agrup.group_by(\"Producto\", maintain_order=True)\n",
        "gb_producto_ordenado"
      ],
      "metadata": {
        "id": "7N5L12_dOZRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Aplicando Agregaciones con agg() y Expresiones**\n",
        "\n",
        "El método `agg()` se aplica al objeto `GroupBy` y toma una lista de expresiones de Polars que definen qué y cómo agregar.\n",
        "\n",
        "## 1. **Agregación simple en una columna:**"
      ],
      "metadata": {
        "id": "MQrazHlrOaFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular la suma total de 'Cantidad' para cada 'Producto'\n",
        "suma_cantidad_por_producto = gb_producto.agg(\n",
        "    pl.col(\"Cantidad\").sum().alias(\"Cantidad_Total\") # pl.col() para referenciar, .sum() para agregar, .alias() para nombrar\n",
        ")\n",
        "print(\"\\nSuma de Cantidad por Producto:\")\n",
        "print(suma_cantidad_por_producto)\n",
        "\n",
        "# Forma abreviada para agregaciones simples como sum, mean, min, max, etc.\n",
        "# Polars también permite pl.sum(\"NombreColumna\"), pl.mean(\"NombreColumna\"), etc.\n",
        "suma_ingresos_por_producto_corto = gb_producto.agg(\n",
        "    pl.sum(\"Ingresos\").alias(\"Ingresos_Totales\")\n",
        ")\n",
        "print(\"\\nSuma de Ingresos por Producto (forma corta):\")\n",
        "print(suma_ingresos_por_producto_corto)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "3nlAv6-oOxHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. **Múltiples agregaciones sobre la misma columna:**"
      ],
      "metadata": {
        "id": "FyNrzCo7OzvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metricas_ingresos_por_producto = gb_producto.agg([\n",
        "    pl.col(\"Ingresos\").sum().alias(\"Ingresos_Suma\"),\n",
        "    pl.col(\"Ingresos\").mean().alias(\"Ingresos_Media\"),\n",
        "    pl.col(\"Ingresos\").min().alias(\"Ingresos_Min\"),\n",
        "    pl.col(\"Ingresos\").max().alias(\"Ingresos_Max\"),\n",
        "    pl.col(\"Ingresos\").count().alias(\"Conteo_Transacciones_Ingresos\") # Cuenta no nulos en Ingresos\n",
        "])\n",
        "print(\"Múltiples métricas de Ingresos por Producto:\")\n",
        "print(metricas_ingresos_por_producto)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "D2DOJIgjOyks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. **Agregaciones sobre diferentes columnas:**\n",
        "Simplemente listas todas las expresiones de agregación que necesitas\n",
        "\n",
        " > * **`pl.len()` vs `pl.count()`:**\n",
        "    * `pl.len()` (o `pl.col(\"cualquier_columna\").len()`) dentro de `agg` te da el número de filas en el grupo (similar a `size()` en Pandas `groups`).\n",
        "    * `pl.count(\"nombre_columna\")` (o `pl.col(\"nombre_columna\").count()`) dentro de `agg` te da el número de valores no nulos en esa columna específica para el grupo. Si solo usas `pl.count()` sin especificar columna, actúa como `pl.len().` Para evitar ambigüedad, `pl.len()` es más claro para el tamaño del grupo."
      ],
      "metadata": {
        "id": "pgEssiTWPmp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resumen_por_region = df_pl_agrup.group_by(\"Region\", maintain_order=True).agg([\n",
        "    pl.col(\"Cantidad\").sum().alias(\"Cantidad_Total_Region\"),\n",
        "    pl.col(\"Ingresos\").mean().alias(\"Ingresos_Promedio_Region\"),\n",
        "    pl.col(\"Valoracion\").median().alias(\"Valoracion_Mediana_Region\"),\n",
        "    pl.col(\"Producto\").n_unique().alias(\"Productos_Unicos_Region\"), # Cuántos productos únicos hay por región\n",
        "    pl.len().alias(\"Numero_Transacciones_Region\") # pl.len() cuenta el número de filas en cada grupo\n",
        "])\n",
        "print(\"Resumen Agregado por Región:\")\n",
        "print(resumen_por_region)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "sZ6X-CmLPtM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. **Agregación Condicional:**\n",
        "Puedes usar `.filter()` dentro de una expresión de agregación."
      ],
      "metadata": {
        "id": "iKP6Bv8pQp92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Suma de ingresos solo de valoraciones altas (>= 4) por Producto\n",
        "ingresos_valoracion_alta_por_producto = df_pl_agrup.group_by(\"Producto\").agg(\n",
        "    pl.col(\"Ingresos\").filter(pl.col(\"Valoracion\") >= 4).sum().alias(\"Ingresos_Valoracion_Alta\")\n",
        ")\n",
        "print(\"Suma de Ingresos de Valoraciones Altas (>=4) por Producto:\")\n",
        "print(ingresos_valoracion_alta_por_producto)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "2XMe8NnhRUmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. **Otras expresiones de agregación útiles:**\n",
        "\n",
        "* `.first()`: Primer elemento del grupo.\n",
        "* `.last():` Último elemento del grupo.\n",
        "* `.std()`: Desviación estándar.\n",
        "* `.var()`: Varianza.\n",
        "* `.quantile(q)`: Calcula el cuantil q.\n",
        "* `.median()`: Equivalente a .quantile(0.5)."
      ],
      "metadata": {
        "id": "UssGa9V6RWKD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ee9dNO6UR4jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pivot Tables con `df.pivot()` (Relacionado con Agrupación):**\n",
        "\n",
        "Mientras que `group_by().agg() `te da una tabla donde los grupos son filas, `pivot()` te permite remodelar los datos de forma que algunos valores de una columna se conviertan en nuevas columnas, y se realiza una agregación. Es similar a las tablas dinámicas."
      ],
      "metadata": {
        "id": "Hok4icQsR7Ga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Pivot Table ---\")\n",
        "# Queremos ver la suma de 'Cantidad' por cada 'Producto' (índice/filas)\n",
        "# y para cada 'Region' (nuevas columnas)\n",
        "pivot_cantidad_producto_region = df_pl_agrup.pivot(\n",
        "    values=\"Cantidad\",    # Columna cuyos valores llenarán la tabla\n",
        "    index=\"Producto\",     # Columna que se convertirá en el índice (filas)\n",
        "    columns=\"Region\",     # Columna cuyos valores únicos se convertirán en nuevas columnas\n",
        "    aggregate_function=\"sum\", # Cómo agregar 'Cantidad' si hay múltiples valores para una celda Producto-Region\n",
        "    sort_columns=True     # Ordenar las nuevas columnas (Regiones)\n",
        ")\n",
        "print(\"Pivot Table: Suma de Cantidad por Producto y Región:\")\n",
        "print(pivot_cantidad_producto_region)\n",
        "\n",
        "# Si no especificas aggregate_function y hay múltiples valores, tomará el primero.\n",
        "# Es mejor ser explícito. Si quieres la media de ingresos:\n",
        "pivot_media_ingresos = df_pl_agrup.pivot(\n",
        "    values=\"Ingresos\",\n",
        "    index=\"Producto\",\n",
        "    columns=\"Region\",\n",
        "    aggregate_function=\"mean\",\n",
        "    sort_columns=True\n",
        ")\n",
        "print(\"\\nPivot Table: Media de Ingresos por Producto y Región:\")\n",
        "print(pivot_media_ingresos) # Observa los nulls donde no hay combinación\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "fStaBBxVSzxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Combinar y Fusionar diferentes DataFrames.\n",
        "\n",
        "Al igual que con Pandas, en Polars a menudo necesitarás combinar datos que residen en diferentes DataFrames. Polars ofrece maneras eficientes y expresivas para lograr esto. Las operaciones principales son la concatenación (para \"pegar\" DataFrames) y los joins (para combinar DataFrames basándose en columnas clave, similar a SQL).\n",
        "\n",
        "## 1. Concatenación con `pl.concat()`\n",
        "\n",
        "La función pl.concat() se utiliza para unir múltiples DataFrames.\n",
        "\n",
        "* Creando DataFrames de ejemplo para concatenar:"
      ],
      "metadata": {
        "id": "S2RewQqtS1gu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "\n",
        "df_pl_c1 = pl.DataFrame({\n",
        "    'id': [1, 2, 3],\n",
        "    'nombre': ['Ana', 'Luis', 'Eva'],\n",
        "    'valor_a': [10, 12, 14]\n",
        "})\n",
        "\n",
        "df_pl_c2 = pl.DataFrame({\n",
        "    'id': [4, 5],\n",
        "    'nombre': ['Juan', 'Sara'],\n",
        "    'valor_a': [16, 18]\n",
        "})\n",
        "\n",
        "# DataFrame con una columna adicional y nombres ligeramente diferentes para mostrar manejo\n",
        "df_pl_c3 = pl.DataFrame({\n",
        "    'id': [6, 7],\n",
        "    'nombre_completo': ['Carlos Rey', 'Laura Paz'], # Nombre de columna diferente\n",
        "    'valor_a': [20, 22],\n",
        "    'valor_b': [100, 102] # Nueva columna\n",
        "})\n",
        "\n",
        "print(\"df_pl_c1:\")\n",
        "print(df_pl_c1)\n",
        "print(\"\\ndf_pl_c2:\")\n",
        "print(df_pl_c2)\n",
        "print(\"\\ndf_pl_c3:\")\n",
        "print(df_pl_c3)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "FGM9biHrYQHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. **Concatenación Vertical (apilar filas, `how='vertical'` es una opción, por defecto intenta alinear):**\n",
        "\n",
        "Polars intentará alinear las columnas por nombre. Si una columna existe en un DataFrame pero no en otro, se rellenará con null en el DataFrame donde falta.\n",
        "\n",
        "> **Nota:** El comportamiento exacto y los mejores parámetros para how en pl.concat pueden evolucionar. `how='vertical_relaxed'` o `how='diagonal_relaxed'` son generalmente buenos para unir DataFrames con schemas que no son idénticos pero que deben apilarse."
      ],
      "metadata": {
        "id": "2ChJef2Qauro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Concatenación Vertical ---\")\n",
        "# Concatenar df_pl_c1 y df_pl_c2 (mismas columnas)\n",
        "df_concat_vertical_simple = pl.concat([df_pl_c1, df_pl_c2]) # how='vertical' es implícito\n",
        "print(\"\\nConcatenación vertical de df_pl_c1 y df_pl_c2:\")\n",
        "print(df_concat_vertical_simple)\n",
        "\n",
        "# Concatenar los tres DataFrames. df_pl_c3 tiene columnas diferentes.\n",
        "# Por defecto, `pl.concat` (cuando se infiere vertical) alinea por nombre y rellena con nulls.\n",
        "df_concat_todos = pl.concat([df_pl_c1, df_pl_c2, df_pl_c3], how=\"diagonal_relaxed\")\n",
        "# `how=\"diagonal_relaxed\"` o `how=\"vertical_relaxed\"` son más explícitos\n",
        "# para cuando los schemas no son idénticos y quieres que se unan inteligentemente.\n",
        "# Simplemente `pl.concat` a menudo hace lo correcto si los tipos son compatibles o pueden ser promocionados.\n",
        "# Para unir y rellenar columnas faltantes con null, `how='diagonal'` o `how='vertical_relaxed'` es robusto.\n",
        "# A partir de versiones recientes, `how=\"vertical_relaxed\"` es una buena opción para unir schemas que pueden no coincidir perfectamente.\n",
        "\n",
        "print(\"\\nConcatenación vertical de df_pl_c1, df_pl_c2, y df_pl_c3 (con `how='diagonal_relaxed'`):\")\n",
        "print(df_concat_todos)\n",
        "# Verás 'nombre' y 'nombre_completo' como columnas separadas, y 'valor_b' con nulls para df_pl_c1 y df_pl_c2\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "8glqd9g1a3FX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. **Concatenación Horizontal `(how='horizontal')`:**\n",
        "\"Pega\" los DataFrames uno al lado del otro, añadiendo columnas. Esto requiere que los DataFrames tengan el mismo número de filas."
      ],
      "metadata": {
        "id": "BzS-pET2a3_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Concatenación Horizontal ---\")\n",
        "df_pl_h1 = pl.DataFrame({\n",
        "    'A': ['A0', 'A1', 'A2'],\n",
        "    'B': ['B0', 'B1', 'B2']\n",
        "})\n",
        "df_pl_h2 = pl.DataFrame({\n",
        "    'C': ['C0', 'C1', 'C2'],\n",
        "    'D': ['D0', 'D1', 'D2']\n",
        "})\n",
        "df_pl_h3_filas_distintas = pl.DataFrame({'E': ['E0', 'E1']})\n",
        "\n",
        "\n",
        "df_concat_horizontal = pl.concat([df_pl_h1, df_pl_h2], how='horizontal')\n",
        "print(\"\\nConcatenación horizontal de df_pl_h1 y df_pl_h2:\")\n",
        "print(df_concat_horizontal)\n",
        "\n",
        "# Esto daría un error porque el número de filas no coincide:\n",
        "try:\n",
        "    df_concat_horizontal_error = pl.concat([df_pl_h1, df_pl_h3_filas_distintas], how='horizontal')\n",
        "except Exception as e:\n",
        "    print(f\"\\nError al concatenar horizontalmente con diferente número de filas: {e}\")\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "tjBZTfjbbKcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Fusión tipo SQL con `df_left.join(df_right, ...)`:**\n",
        "\n",
        "El método `.join()` en un DataFrame de Polars es la principal forma de realizar fusiones basadas en claves, similar a los JOIN de SQL.\n",
        "\n",
        "## 1. **Creando DataFrames de ejemplo para join:**"
      ],
      "metadata": {
        "id": "iJhmYdZLbNkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_empleados = pl.DataFrame({\n",
        "    'ID_Empleado': [1, 2, 3, 4, 5],\n",
        "    'Nombre': ['Carlos', 'Laura', 'Pedro', 'Sofia', 'David'],\n",
        "    'ID_Depto': [101, 102, 101, 103, 102]\n",
        "})\n",
        "\n",
        "df_departamentos = pl.DataFrame({\n",
        "    'ID_Depto': [101, 102, 103, 104],\n",
        "    'Nombre_Depto': ['Ventas', 'Marketing', 'IT', 'Recursos Humanos'],\n",
        "    'Ubicacion': ['Piso 1', 'Piso 2', 'Piso 3', 'Piso 1']\n",
        "})\n",
        "\n",
        "df_proyectos = pl.DataFrame({\n",
        "    'ID_Proyecto': ['P1', 'P2', 'P3', 'P4'],\n",
        "    'ID_Empleado_Resp': [1, 2, 1, 5], # Usaremos esta para unir con ID_Empleado\n",
        "    'Nombre_Proyecto': ['Alpha', 'Beta', 'Gamma', 'Delta']\n",
        "})\n",
        "\n",
        "print(\"df_empleados:\")\n",
        "print(df_empleados)\n",
        "print(\"\\ndf_departamentos:\")\n",
        "print(df_departamentos)\n",
        "print(\"\\ndf_proyectos:\")\n",
        "print(df_proyectos)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "WCWRNVdQbY1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. **Tipos de Join (`how`):**\n",
        "El método join toma el DataFrame \"derecho\" como primer argumento y luego los parámetros de unión.\n",
        "\n",
        "* `how='inner'` (por defecto): Solo filas donde la clave de unión existe en ambos DataFrames."
      ],
      "metadata": {
        "id": "5icVsooXbbQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Joins tipo SQL ---\")\n",
        "# Inner join entre empleados y departamentos\n",
        "df_inner_join = df_empleados.join(df_departamentos, on=\"ID_Depto\")\n",
        "print(\"\\nInner Join de empleados y departamentos (en 'ID_Depto'):\")\n",
        "print(df_inner_join)\n",
        "# Depto 104 (Recursos Humanos) no aparece porque ningún empleado está asignado a él."
      ],
      "metadata": {
        "id": "DPkmwr_1bk7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  * `how='left'`: Todas las filas del DataFrame izquierdo (df_empleados en este caso) y las coincidentes del derecho."
      ],
      "metadata": {
        "id": "UtNJ10k3bl7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_left_join = df_empleados.join(df_departamentos, on=\"ID_Depto\", how=\"left\")\n",
        "print(\"\\nLeft Join de empleados y departamentos:\")\n",
        "print(df_left_join)\n",
        "# Todos los empleados aparecen. Si un ID_Depto no existe en df_departamentos, las cols de depto serán null."
      ],
      "metadata": {
        "id": "nnYIm2mKbstt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  * `how='outer'`: Todas las filas de ambos DataFrames. Si no hay coincidencia, se rellena con null."
      ],
      "metadata": {
        "id": "WQOhGlTGbvz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_outer_join = df_empleados.join(df_departamentos, on=\"ID_Depto\", how=\"outer\")\n",
        "print(\"\\nOuter Join de empleados y departamentos:\")\n",
        "print(df_outer_join)\n",
        "# Todos los empleados y todos los departamentos aparecen."
      ],
      "metadata": {
        "id": "3ilpcvsEbzim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  * `how='semi'`: Devuelve solo las filas del DataFrame izquierdo que tienen una coincidencia en el DataFrame derecho. No incluye columnas del DataFrame derecho."
      ],
      "metadata": {
        "id": "Bnb19ihlb10x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Empleados que están en departamentos que SÍ existen en df_departamentos\n",
        "# (En este caso, todos los empleados tienen un ID_Depto que existe en df_departamentos)\n",
        "df_semi_join = df_empleados.join(df_departamentos, on=\"ID_Depto\", how=\"semi\")\n",
        "print(\"\\nSemi Join (empleados con departamento existente):\")\n",
        "print(df_semi_join)"
      ],
      "metadata": {
        "id": "9JBqhWJtb6zJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  * `how='anti'`: Devuelve solo las filas del DataFrame *izquierdo* que NO tienen una coincidencia en el DataFrame *derecho*."
      ],
      "metadata": {
        "id": "fGfyD183b9Ou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. **Join con claves de nombres diferentes (`left_on`, `right_on`):**"
      ],
      "metadata": {
        "id": "luEpjF2ecD-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unir empleados con sus proyectos\n",
        "# df_empleados tiene 'ID_Empleado', df_proyectos tiene 'ID_Empleado_Resp'\n",
        "df_empleados_proyectos = df_empleados.join(\n",
        "    df_proyectos,\n",
        "    left_on=\"ID_Empleado\",\n",
        "    right_on=\"ID_Empleado_Resp\",\n",
        "    how=\"left\" # Queremos todos los empleados y sus proyectos si los tienen\n",
        ")\n",
        "print(\"Join de empleados y proyectos con claves de nombres diferentes (left join):\")\n",
        "print(df_empleados_proyectos)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "Ps-xBFD6cIJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. **Join con múltiples claves:**\n",
        "Pasa una lista de strings a on, `left_on`, o `right_on`.\n",
        "\n",
        "```\n",
        "# Ejemplo conceptual (necesitaríamos DFs adecuados)\n",
        "df_A = pl.DataFrame({'k1': [1,1,2], 'k2': ['a','b','a'], 'valA': [10,20,30]})\n",
        "\n",
        "df_B = pl.DataFrame({'k1': [1,1,2], 'k2': ['b','a','a'], 'valB': [100,200,300]})\n",
        "\n",
        "df_multi_key_join = df_A.join(df_B, on=[\"k1\", \"k2\"], how=\"inner\")\n",
        "\n",
        "print(\"\\nJoin con múltiples claves ('k1', 'k2'):\")\n",
        "print(df_multi_key_join)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "DlkptY9KcI_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. **Manejo de columnas con nombres duplicados (no claves) con suffix:**\n",
        "Si después del `join`, hay columnas (que no son las claves de unión) con el mismo nombre provenientes de ambos DataFrames, Polars añadirá un sufijo al nombre de la columna del DataFrame *derecho*. Puedes personalizar este sufijo."
      ],
      "metadata": {
        "id": "p3C2ki69cZL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_deptos_con_nombre = df_departamentos.rename({\"Nombre_Depto\": \"Nombre\"}) # Renombramos para que coincida con 'Nombre' de empleados\n",
        "\n",
        "# Ahora df_empleados y df_deptos_con_nombre tienen una columna 'Nombre'\n",
        "df_join_sufijo = df_empleados.join(\n",
        "    df_deptos_con_nombre,\n",
        "    on=\"ID_Depto\",\n",
        "    how=\"left\",\n",
        "    suffix=\"_Depto\" # Se añadirá a las columnas del DF derecho (df_deptos_con_nombre) si hay colisión\n",
        ")\n",
        "print(\"Join con sufijo para columnas duplicadas no clave:\")\n",
        "print(df_join_sufijo) # Verás 'Nombre' (del empleado) y 'Nombre_Depto' (del departamento)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "xEgjcMhbckGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Breve Mención de `join_asof`**\n",
        "\n",
        "Polars también ofrece `join_asof` para uniones \"a partir de\" o \"más cercanas\".\n",
        "\n",
        "Es muy útil en series temporales para unir observaciones basándose en la marca de tiempo más cercana (pero no posterior, o dentro de una tolerancia).\n",
        "\n",
        "\n",
        "```\n",
        "# Ejemplo conceptual:\n",
        "df_trades = pl.DataFrame({\n",
        "     \"time\": [datetime(2025,1,1,9,0,0), datetime(2025,1,1,9,0,5), datetime(2025,1,1,9,0,12)],\n",
        "     \"price\": [100, 101, 100.5], \"stock\": [\"A\", \"A\", \"A\"]\n",
        "  })\n",
        "\n",
        "df_quotes = pl.DataFrame({\n",
        "     \"time\": [datetime(2025,1,1,9,0,0), datetime(2025,1,1,9,0,4), datetime(2025,1,1,9,0,10), datetime(2025,1,1,9,0,11)],\n",
        "     \"bid\": [99.9, 99.8, 100.4, 100.3], \"ask\": [100.1, 100.0, 100.6, 100.5], \"stock\": [\"A\", \"A\", \"A\", \"A\"]\n",
        "  })\n",
        "\n",
        "# Unir cada trade con la cotización más reciente ANTES o EN el momento del trade.\n",
        "df_trades_con_quotes = df_trades.join_asof(df_quotes, on=\"time\", by=\"stock\", strategy=\"backward\")\n",
        "\n",
        "print(df_trades_con_quotes)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Ql0-tAyZcn0P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **Entrada y Salida de Datos (I/O).**\n",
        "\n",
        "Saber cómo cargar datos eficientemente en Polars y cómo guardar tus resultados es tan crucial como la manipulación misma.\n",
        "\n",
        "Polars destaca en esto, especialmente con su API scan_* para trabajar con datasets más grandes que la memoria RAM (modo `lazy`) y su excelente soporte para formatos columnares como `Parquet`.\n",
        "\n",
        "## 1. **Leer Datos desde Archivos**\n",
        "\n",
        "Las funciones de lectura en Polars generalmente comienzan con pl.read_* (para modo eager, carga todo en memoria) o pl.scan_* (para modo lazy, crea un plan).\n",
        "\n",
        "### **Leer Archivos CSV**\n",
        "\n",
        "  * `pl.read_csv()` (Modo `Eager`): Carga el CSV completo en un DataFrame en memoria."
      ],
      "metadata": {
        "id": "6eoesJISdToo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import io # Para simular archivos\n",
        "\n",
        "# Simular contenido de un archivo CSV\n",
        "csv_data_polars = \"\"\"id,nombre,valor,categoria\n",
        "1,Manzana,1.2,Fruta\n",
        "2,Leche,0.9,Lácteo\n",
        "3,Pan,2.0,Panadería\n",
        "4,Naranja,N/A,Fruta\n",
        "\"\"\"\n",
        "# En un caso real: df_eager_csv = pl.read_csv(\"ruta/a/archivo.csv\")\n",
        "\n",
        "df_eager_csv = pl.read_csv(\n",
        "    source=io.StringIO(csv_data_polars), # Usamos StringIO para leer la cadena\n",
        "    has_header=True,\n",
        "    separator=',',\n",
        "    null_values=\"N/A\", # Definir qué strings son nulos\n",
        "    dtypes={\"valor\": pl.Float32, \"id\": pl.Int16} # Especificar tipos\n",
        ")\n",
        "print(\"--- Leer CSV (Modo Eager) ---\")\n",
        "print(\"DataFrame desde read_csv:\")\n",
        "print(df_eager_csv)\n",
        "print(df_eager_csv.schema)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "jLRP4yhmctzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  * `pl.scan_csv()` (**Modo Lazy - ¡Muy Recomendado para Archivos Grandes!**):\n",
        "\n",
        "No carga los datos inmediatamente. En su lugar, escanea el archivo para inferir el schema y crea un LazyFrame (un plan de cómo cargar y procesar los datos). Las operaciones se ejecutan solo cuando llamas a `.collect()`."
      ],
      "metadata": {
        "id": "uLn7abhNd1Gw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# En un caso real: lf_lazy_csv = pl.scan_csv(\"ruta/a/archivo_grande.csv\")\n",
        "lf_lazy_csv = pl.scan_csv(\n",
        "    source=io.StringIO(csv_data_polars), # Usamos la misma cadena\n",
        "    has_header=True,\n",
        "    separator=',',\n",
        "    null_values=\"N/A\",\n",
        "    # dtypes se puede especificar, o Polars intentará inferirlo (con with_infer_schema_length)\n",
        "    infer_schema_length=100 # Cuántas filas escanear para inferir el schema\n",
        ")\n",
        "print(\"--- Leer CSV (Modo Lazy con scan_csv) ---\")\n",
        "print(\"Tipo de objeto después de scan_csv:\", type(lf_lazy_csv))\n",
        "# <class 'polars.lazyframe.frame.LazyFrame'>\n",
        "\n",
        "# Construyes tu consulta en el LazyFrame\n",
        "query_lazy = (\n",
        "    lf_lazy_csv\n",
        "    .filter(pl.col(\"valor\") > 1.0)\n",
        "    .select([\"nombre\", \"categoria\", pl.col(\"valor\") * 2 .alias(\"valor_duplicado\")])\n",
        "    .group_by(\"categoria\")\n",
        "    .agg(pl.col(\"valor_duplicado\").sum())\n",
        ")\n",
        "print(\"\\nPlan de consulta (LazyFrame):\")\n",
        "print(query_lazy) # Muestra el plan lógico\n",
        "\n",
        "# Ejecutas la consulta y obtienes el DataFrame resultante con .collect()\n",
        "df_resultado_lazy = query_lazy.collect()\n",
        "print(\"\\nDataFrame resultado después de .collect():\")\n",
        "print(df_resultado_lazy)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "OsmZHIvCeAEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Beneficios de scan_csv (y otros `scan_`*):**\n",
        "\n",
        "\n",
        "* Optimización de Consultas: Polars puede optimizar toda la cadena de operaciones antes de tocar los datos.\n",
        "* Out-of-Core: Puede procesar archivos más grandes que tu RAM porque solo carga en memoria los trozos (chunks) que necesita en cada momento.\n",
        "* Proyección de Columnas (Predicate Pushdown): Si solo seleccionas ciertas columnas o filtras filas, Polars intentará leer solo los datos necesarios del disco.\n",
        "\n",
        "## 2. **Leer Archivos Excel con `pl.read_excel()`:**\n",
        "Polars usa motores como calamine (escrito en Rust, a menudo el predeterminado y muy rápido) o xlsx2csv para leer archivos Excel.\n",
        "Asegúrate de tener el soporte para Excel instalado (`pip install polars[excel]`)."
      ],
      "metadata": {
        "id": "iu2D9hbIeGEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simular un archivo Excel (creando un DF y guardándolo en un buffer de bytes)\n",
        "df_para_excel_pl = pl.DataFrame({\n",
        "    \"Producto\": [\"X\", \"Y\", \"Z\"],\n",
        "    \"Ventas_2025\": [100, 150, 120],\n",
        "    \"Ventas_2024\": [90, 140, 110]\n",
        "})\n",
        "buffer_excel = io.BytesIO()\n",
        "# Polars no tiene un ExcelWriter tan complejo como Pandas para múltiples hojas directamente en write_excel.\n",
        "# Para múltiples hojas, podrías escribir varios CSVs y luego unirlos, o usar una librería específica de Excel.\n",
        "# Aquí escribimos una sola hoja.\n",
        "df_para_excel_pl.write_excel(\n",
        "    workbook=buffer_excel, # Puede ser una ruta o un buffer\n",
        "    worksheet=\"Reporte_Ventas\"\n",
        ")\n",
        "buffer_excel.seek(0) # Regresar el puntero para leer\n",
        "\n",
        "# En un caso real: df_excel = pl.read_excel(\"ruta/a/archivo.xlsx\", sheet_name=\"Hoja1\")\n",
        "df_excel_leido = pl.read_excel(\n",
        "    source=buffer_excel,\n",
        "    sheet_name=\"Reporte_Ventas\", # También puede ser un índice (0, 1, ...)\n",
        "    engine='calamine' # 'calamine' es rápido y no requiere MS Excel\n",
        ")\n",
        "print(\"--- Leer Excel ---\")\n",
        "print(\"DataFrame leído desde Excel simulado:\")\n",
        "print(df_excel_leido)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "rxKOfXBkegSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. **Leer Archivos Parquet con `pl.read_parquet()` y `pl.scan_parquet()`**\n",
        "Parquet es un formato de archivo columnar altamente eficiente, optimizado para almacenamiento y análisis rápido.\n",
        "\n",
        "Es el formato preferido para trabajar con Polars (y muchas otras herramientas de Big Data)."
      ],
      "metadata": {
        "id": "DVAAkJgyejJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simular un archivo Parquet\n",
        "df_para_parquet = pl.DataFrame({\n",
        "    \"id\": [1, 2, 3],\n",
        "    \"data\": [\"a\", \"b\", \"c\"],\n",
        "    \"value\": [10.1, 11.2, 12.3]\n",
        "})\n",
        "buffer_parquet = io.BytesIO()\n",
        "df_para_parquet.write_parquet(buffer_parquet)\n",
        "buffer_parquet.seek(0)\n",
        "\n",
        "# Leer en modo eager\n",
        "# En un caso real: df_parquet_eager = pl.read_parquet(\"ruta/a/archivo.parquet\")\n",
        "df_parquet_eager = pl.read_parquet(buffer_parquet)\n",
        "print(\"--- Leer Parquet (Eager y Lazy) ---\")\n",
        "print(\"DataFrame desde read_parquet (eager):\")\n",
        "print(df_parquet_eager)\n",
        "\n",
        "# Leer en modo lazy (MUY recomendado para Parquet)\n",
        "buffer_parquet.seek(0) # Rebobinar el buffer para scan\n",
        "# En un caso real: lf_parquet_lazy = pl.scan_parquet(\"ruta/a/archivo_grande.parquet\")\n",
        "lf_parquet_lazy = pl.scan_parquet(buffer_parquet)\n",
        "print(\"\\nLazyFrame desde scan_parquet:\")\n",
        "print(lf_parquet_lazy) # Muestra el plan\n",
        "\n",
        "df_resultado_parquet_lazy = lf_parquet_lazy.filter(pl.col(\"value\") > 11.0).collect()\n",
        "print(\"\\nResultado de scan_parquet después de filter y collect:\")\n",
        "print(df_resultado_parquet_lazy)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "cg19wKyye1sD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. **Mención de Otros Formatos de Lectura:**\n",
        "\n",
        "* `pl.read_json()` / `pl.scan_ndjson()`: Para archivos JSON o Newline Delimited JSON.\n",
        "* `pl.read_ipc()` / `pl.scan_ipc()`: Para formato Arrow IPC (Feather).\n",
        "* Conectores para bases de datos (a menudo mediante librerías de terceros o adbc):\n",
        "  * Polars puede leer desde bases de datos SQL, aunque la forma exacta puede depender del conector (`pip install polars[adbc]` `polars[connectorx]`).\n",
        "  Por ejemplo, `pl.read_database(query=\"SELECT * FROM mi_tabla\", connection=mi_conexion_uri)`.\n",
        "\n",
        "## **Escribir Datos a Archivos:**\n",
        "\n",
        "Los métodos para escribir DataFrames a archivos suelen ser `df.write_`*.\n",
        "\n",
        "### 1. **Escribir a Archivos CSV con `df.write_csv()`:**"
      ],
      "metadata": {
        "id": "kztFVGM_e3UR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_para_escribir = pl.DataFrame({\n",
        "    \"colA\": [1, 2, 3], \"colB\": [\"x\", \"y\", \"z\"]\n",
        "})\n",
        "print(\"--- Escribir a Archivos ---\")\n",
        "print(\"\\nDataFrame para escribir:\")\n",
        "print(df_para_escribir)\n",
        "\n",
        "buffer_escritura_csv_pl = io.StringIO()\n",
        "df_para_escribir.write_csv(buffer_escritura_csv_pl, separator=\";\")\n",
        "print(\"\\nContenido CSV 'guardado' en buffer (separador ';'):\")\n",
        "print(buffer_escritura_csv_pl.getvalue())\n",
        "# En un caso real: df_para_escribir.write_csv(\"salida.csv\", separator=\";\")"
      ],
      "metadata": {
        "id": "4AQ_-IDefp2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. **Escribir a Archivos Excel con `df.write_excel()`**\n",
        "Requiere tener un motor como xlsxwriter instalado (`pip install xlsxwriter`)."
      ],
      "metadata": {
        "id": "4d5IgNeYfstq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "buffer_escritura_excel_pl = io.BytesIO()\n",
        "try:\n",
        "    df_para_escribir.write_excel(\n",
        "        workbook=buffer_escritura_excel_pl,\n",
        "        worksheet=\"MisDatosPolars\",\n",
        "        # Si quieres opciones de formato, la API es más limitada que la de Pandas\n",
        "        # o podrías usar xlsxwriter directamente con los datos de Polars.\n",
        "    )\n",
        "    print(\"\\n(Simulación) DataFrame 'guardado' en buffer de Excel.\")\n",
        "    # Para verificar: buffer_escritura_excel_pl.seek(0); df_leido = pl.read_excel(buffer_escritura_excel_pl)\n",
        "except Exception as e:\n",
        "    print(f\"\\nError escribiendo Excel (puede necesitar 'xlsxwriter'): {e}\")\n",
        "# En un caso real: df_para_escribir.write_excel(\"salida.xlsx\", worksheet=\"HojaPrincipal\")"
      ],
      "metadata": {
        "id": "NEBTyQLxfzZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. **Escribir a Archivos Parquet con `df.write_parquet()`:**\n",
        "Este es el método recomendado para guardar DataFrames de Polars si vas a volver a leerlos con Polars u otras herramientas compatibles con Arrow/Parquet, debido a su eficiencia."
      ],
      "metadata": {
        "id": "fBeQIC8ef0cd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "buffer_escritura_parquet_pl = io.BytesIO()\n",
        "df_para_escribir.write_parquet(buffer_escritura_parquet_pl, compression=\"snappy\") # snappy es una compresión rápida\n",
        "print(\"\\n(Simulación) DataFrame 'guardado' en buffer de Parquet con compresión snappy.\")\n",
        "# En un caso real: df_para_escribir.write_parquet(\"salida.parquet\", compression=\"snappy\")\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "oBMubGgigCNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Interoperabilidad con Pandas, NumPy y PyArrow**\n",
        "\n",
        "Polars está diseñado para coexistir y trabajar bien con otras bibliotecas del ecosistema PyData."
      ],
      "metadata": {
        "id": "zWN5x7qhgFlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Interoperabilidad ---\")\n",
        "# DataFrame de Polars de ejemplo\n",
        "df_polars_interop = pl.DataFrame({\"a\": [1, 2], \"b\": [3.0, 4.5]})\n",
        "\n",
        "# 1. A Pandas DataFrame\n",
        "df_pandas_convertido = df_polars_interop.to_pandas()\n",
        "print(\"\\nConvertido a Pandas DataFrame:\")\n",
        "print(df_pandas_convertido)\n",
        "print(type(df_pandas_convertido))\n",
        "\n",
        "# 2. Desde un Pandas DataFrame\n",
        "df_pandas_original = pd.DataFrame({\"x\": [10, 20], \"y\": [\"hola\", \"mundo\"]})\n",
        "df_polars_desde_pandas = pl.from_pandas(df_pandas_original)\n",
        "print(\"\\nConvertido desde Pandas DataFrame:\")\n",
        "print(df_polars_desde_pandas)\n",
        "print(type(df_polars_desde_pandas))\n",
        "\n",
        "# 3. A NumPy array (puede ser una o varias columnas)\n",
        "array_numpy = df_polars_interop.to_numpy() # Devuelve un array estructurado si hay múltiples tipos\n",
        "print(\"\\nConvertido a NumPy array (estructurado):\")\n",
        "print(array_numpy)\n",
        "# Para una sola columna:\n",
        "array_numpy_col_a = df_polars_interop[\"a\"].to_numpy()\n",
        "print(\"\\nColumna 'a' como NumPy array:\")\n",
        "print(array_numpy_col_a)\n",
        "\n",
        "# 4. A tabla PyArrow\n",
        "tabla_arrow = df_polars_interop.to_arrow()\n",
        "print(\"\\nConvertido a tabla PyArrow:\")\n",
        "print(tabla_arrow)\n",
        "print(type(tabla_arrow))\n",
        "\n",
        "# 5. Desde tabla PyArrow\n",
        "df_polars_desde_arrow = pl.from_arrow(tabla_arrow)\n",
        "print(\"\\nConvertido desde tabla PyArrow:\")\n",
        "print(df_polars_desde_arrow)"
      ],
      "metadata": {
        "id": "zBDiEQfTgI81"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}